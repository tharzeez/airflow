from airflow import DAG, XComArg
from airflow.operators.bash import BashOperator
from airflow.providers.common.sql.operators.sql import SQLExecuteQueryOperator
import pendulum
import dateutil.relativedelta
from datetime import datetime, date
import calendar
import logging
from airflow.decorators import task
from airflow.operators.python import get_current_context

# Observations: If a task is failed and restarted by clearing and the state depends on the previous tasks,
# the state value is obtained from the previous calculations.

# TODO: Insert created_at field from the code logic instead of calculating runtime in MySQL.
missing_date_query = """
insert into missing_entries (user_id, date_value, day_value) with user_report as (
  SELECT 
    sum(log_hours) as combined_total_hours, 
    start_date, 
    date_format(start_date, "%%W") as day, 
    user_id, 
    first_name 
  FROM 
    timesheets 
    inner join users using (user_id) 
  where 
    user_id = %s 
    and start_date >= %s 
    and start_date <= %s 
  group by 
    start_date 
  union all 
  -- This returns a single row with the starting date so that we we will have atleast one data in the left table
  -- So that when we right join we will have the data, this is when there is no entry and the missing entries just skip.
  select 
    sum(null) as combined_total_hours, 
    %s as start_date, 
    null as day, 
    %s as user_id, 
    null as first_name 
  FROM 
    timesheets 
  where 
    user_id = %s
    and start_date >= %s
    and start_date <= %s
  having 
    count(*) = 0 
  order by 
    start_date desc
), 
calendar as (
  select 
    date_value, 
    day_value 
  from 
    calendar c 
  where 
    c.date_value >= %s 
    and c.date_value <= %s
  order by 
    date_value desc
), 
total_hours as (
  select 
    IF (
      urbs.combined_total_hours IS NULL, 
      true, false
    ) as is_autogenerated, 
    c.date_value, 
    c.day_value 
  from 
    user_report urbs 
    right outer join calendar c on c.date_value = urbs.start_date 
  order by 
    date_value asc
)
select 
  %s as user_id, 
  t.date_value, 
  t.day_value
from 
  total_hours t 
where 
  t.is_autogenerated = true
  and not exists (select user_id, date_value from missing_entries where user_id = %s and date_value >= %s and date_value <= %s )
"""

# except
# select
#   user_id,
#   date_value,
#   day_value
# from
#   missing_entries
# where
#   user_id = %s

def get_start_end_date(dates):
    print(dates[0], dates[-1], dates)
    return dates[0][0], dates[-1][0]

# Define the DAG
with DAG(
    dag_id='weekly_data_insert_prod',
    # dag_id='weekly_data_insert',
    description='Weekly data insertion in MySQL',
    # schedule_interval = '0 0 2 * *',
    schedule_interval = None,
    start_date=pendulum.datetime(2023, 5, 24, tz="UTC"),
    catchup=False
) as dag:

    logging.debug('This is a debug message')

    # Task 1: Get all the Users from the database
    get_all_users = SQLExecuteQueryOperator(
        task_id='get_all_users',
        # conn_id='local_maxistime',
        conn_id='maxistime_prod',
        database='maxistime',
        sql='SELECT user_id FROM users where company_id = 1 and status = \'active\' and is_deleted = 0',
        dag=dag
    )

    get_next_seven_days = SQLExecuteQueryOperator(
        task_id='get_next_seven_days',
        # conn_id='local_maxistime',
        conn_id='maxistime_prod',
        database='maxistime',
        sql='''SELECT date_format(date_value, "%Y-%m-%d")  from calendar where is_airflow_ran = 0 and date_value > (select date_value from calendar where is_airflow_ran = 1 order by date_value desc limit 1) and date_value <= (select (date_add(date_value, interval 7 day)) from calendar where is_airflow_ran = 1 order by date_value desc limit 1)''',
        # sql='''SELECT date_format(date_value, "%Y-%m-%d")  from calendar where date_value >= "2023-06-01" and date_value <= "2023-06-11" ''',
        dag=dag
    )

    @task
    def create_store_missed_details_kwargs(user_id):
        context = get_current_context()
        dates = context["ti"].xcom_pull(task_ids="get_next_seven_days")
        print('dates', dates)
        flat_user_id = user_id[0]
        date_range_start, date_range_end = get_start_end_date(dates)
        return { "parameters": [ flat_user_id, date_range_start, date_range_end,
                                date_range_start, flat_user_id, flat_user_id, date_range_start,
                                date_range_end, date_range_start, date_range_end, flat_user_id, flat_user_id,
                                date_range_start, date_range_end ] }

    store_missed_details_kwargs = create_store_missed_details_kwargs.expand(user_id = XComArg(get_all_users))

    store_missed_details = SQLExecuteQueryOperator.partial(
        task_id='store_missed_details',
        # conn_id='local_maxistime',
        conn_id='maxistime_prod',
        database='maxistime',
        sql=missing_date_query,
    ).expand_kwargs(store_missed_details_kwargs)


    # https://stackoverflow.com/questions/67631581/airflow-2-0-2-how-to-pass-parameter-within-postgres-tasks-using-xcom
    # The params argument is not "Templated", so it would only render strings. So have to move param directly to SQL
    update_calendar_table = SQLExecuteQueryOperator(
        task_id='update_calendar_table',
        # conn_id='local_maxistime',
        conn_id='maxistime_prod',
        database='maxistime',
        sql='''update calendar set is_airflow_ran = 1 where date_value >= "{{ task_instance.xcom_pull(task_ids="get_next_seven_days")[0][0] }}" and date_value <= "{{ task_instance.xcom_pull(task_ids="get_next_seven_days")[-1][0] }}" ''',
        dag=dag
    )

    success_main = BashOperator(
            task_id='success_main',
            bash_command='echo "success_main"',
    )

    # Define task dependencies
    # get_all_users >> store_missed_details >> success_main
    get_all_users >> get_next_seven_days >> store_missed_details >> update_calendar_table >> success_main
    # store_missed_details >> success_main
